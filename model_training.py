# -*- coding: utf-8 -*-
"""model_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dPPY1nIjcpJWUiUwRu7K6fpl9WUnhwyL
"""

# Step 0: Mount Google Drive to save models and access project files
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/StockPricePrediction

"""Step 1: Install Required Libraries"""

# Step 1: Install all required packages
!pip install yfinance pandas numpy scikit-learn tensorflow textblob pytrends requests python-dotenv plotly joblib

"""Step 2: Import Modules"""

!pip install vaderSentiment

# Step 2: Import required Python modules
import os
import numpy as np
import pandas as pd
import joblib
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Import your backend modules
from backend.stock_data import fetch_historical_prices
from backend.sentiment import fetch_news_sentiment, fetch_twitter_sentiment, fetch_google_trends

from backend.features import create_features

FEATURE_COLUMNS = ['MA10', 'MA50', 'Pct_change', 'Volatility', 'News_sentiment', 'Twitter_sentiment', 'Trend_score']

"""Step 3: Define Configs"""

# Step 3: Configuration for training
SECTOR_STOCKS = {
    "Technology": ["AAPL", "AMZN", "MSFT", "GOOGL", "FB"],
    "Pharma": ["PFE", "JNJ", "MRK", "ABBV"],
    "Banking": ["HDFCBANK.NS", "ICICIBANK.NS", "AXISBANK.NS", "KOTAKBANK.NS", "SBIN.NS"]
}


MODEL_DIR = "models"
WINDOW = 50       # sequence length for LSTM
EPOCHS = 50
BATCH_SIZE = 16
PERIOD = '2y'     # historical period to fetch

# Create folder to save models if it doesn't exist
os.makedirs(MODEL_DIR, exist_ok=True)

"""Step 4: Build LSTM Model Function"""

# Step 4: Function to build LSTM model with multiple layers and dropout
def build_lstm_model(input_shape):
    model = Sequential()
    model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=input_shape))
    model.add(Dropout(0.2))
    model.add(LSTM(60, activation='relu', return_sequences=True))
    model.add(Dropout(0.3))
    model.add(LSTM(80, activation='relu', return_sequences=True))
    model.add(Dropout(0.4))
    model.add(LSTM(50, activation='relu'))
    model.add(Dropout(0.3))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return model

# Step 5: Function to train LSTM for a single stock
def train_lstm_for_ticker(ticker):
    print(f"\n=== TRAINING {ticker} ===")

    # Fetch historical prices and sentiment features
    hist = fetch_historical_prices(ticker, period=PERIOD)
    news = fetch_news_sentiment(ticker)
    twitter = fetch_twitter_sentiment(ticker)
    trends = fetch_google_trends(ticker)

    # Create feature dataframe
    df = create_features(hist, news, twitter, trends)
    if df.empty or len(df) < WINDOW + 10:
        print(f"Not enough data for {ticker}")
        return

    # Prepare feature matrix and scale
    data = df[FEATURE_COLUMNS].copy()
    scaler = MinMaxScaler()
    data_scaled = scaler.fit_transform(data)

    # Train/test split
    split_idx = int(len(data_scaled)*0.8)
    train_scaled = data_scaled[:split_idx]
    test_scaled = data_scaled[split_idx-WINDOW:]  # keep WINDOW for sequence

    # Function to create sequences for LSTM
    def create_sequences(dataset):
        X, y = [], []
        for i in range(WINDOW, len(dataset)-1):
            X.append(dataset[i-WINDOW:i])
            y.append(dataset[i+1, 0])  # next day's Close
        return np.array(X), np.array(y)

    X_train, y_train = create_sequences(train_scaled)
    X_test, y_test = create_sequences(test_scaled)

    print(f"X_train: {X_train.shape}, y_train: {y_train.shape}")
    print(f"X_test: {X_test.shape}, y_test: {y_test.shape}")

    # Build model
    model = build_lstm_model((X_train.shape[1], X_train.shape[2]))

    # Early stopping to prevent overfitting
    es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

    # Train the model
    history = model.fit(
        X_train, y_train,
        validation_data=(X_test, y_test),
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        callbacks=[es],
        verbose=1
    )

    # Evaluate model
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    print(f"{ticker} Evaluation -> MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}")

    # Save model and scaler
    model_path = os.path.join(MODEL_DIR, f"{ticker}_lstm.keras")
    scaler_path = os.path.join(MODEL_DIR, f"{ticker}_scaler.save")
    model.save(model_path)
    joblib.dump(scaler, scaler_path)
    print(f"Saved model to {model_path} and scaler to {scaler_path}")

# Step 6: Train LSTM for all selected stocks in each sector
for sector, tickers in SECTOR_STOCKS.items():
    print(f"\n=== SECTOR: {sector} ===")
    for ticker in tickers:
        train_lstm_for_ticker(ticker)

print("âœ… All sector models trained and saved.")